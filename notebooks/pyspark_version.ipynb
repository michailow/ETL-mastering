{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96998fcd",
   "metadata": {},
   "source": [
    "# Advanced SQL puzzles with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d4e3459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.8\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2bd8ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70decd9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67e6445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdd94ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Puzzles').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf6dfa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6d85bb",
   "metadata": {},
   "source": [
    "### Puzzle #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3d211598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Item |\n",
      "+-----+\n",
      "|Sugar|\n",
      "|Bread|\n",
      "|Juice|\n",
      "|Soda |\n",
      "|Flour|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows1 = [(\"Sugar\", ), \n",
    "        (\"Bread\", ), \n",
    "        (\"Juice\", ), \n",
    "        (\"Soda\", ),\n",
    "        (\"Flour\", )\n",
    "      ]\n",
    "nameColumns1 = [\"Item\"]\n",
    "df1 = spark.createDataFrame(data=rows, schema = nameColumns1)\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "19a84424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Item  |\n",
      "+------+\n",
      "|Sugar |\n",
      "|Bread |\n",
      "|Butter|\n",
      "|Cheese|\n",
      "|Fruit |\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows1_ = [(\"Sugar\", ), \n",
    "          (\"Bread\", ), \n",
    "          (\"Butter\", ), \n",
    "          (\"Cheese\", ),\n",
    "          (\"Fruit\", )\n",
    "         ]\n",
    "nameColumns1_ = [\"Item\"]\n",
    "df1_ = spark.createDataFrame(data=rows1_, schema = nameColumns1_)\n",
    "df1_.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c6f9b9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|Item |Item  |\n",
      "+-----+------+\n",
      "|Bread|Bread |\n",
      "|null |Butter|\n",
      "|null |Cheese|\n",
      "|Flour|null  |\n",
      "|null |Fruit |\n",
      "|Juice|null  |\n",
      "|Soda |null  |\n",
      "|Sugar|Sugar |\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df1_, df1.Item ==  df1_.Item, \"outer\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39361ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29bcd4f2",
   "metadata": {},
   "source": [
    "### Puzzle #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55c5411a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------------+\n",
      "|Customer ID|Type    |Phone Number|\n",
      "+-----------+--------+------------+\n",
      "|1001       |Cellular|555-897-5421|\n",
      "|1001       |Work    |555-897-6542|\n",
      "|1001       |Home    |555-698-9874|\n",
      "|2002       |Cellular|555-963-6544|\n",
      "|2002       |Work    |555-812-9856|\n",
      "|3003       |Cellular|555-987-6541|\n",
      "+-----------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows5 = [(\"1001\", \"Cellular\", \"555-897-5421\"), \n",
    "         (\"1001\", \"Work\", \"555-897-6542\"), \n",
    "         (\"1001\", \"Home\", \"555-698-9874\"), \n",
    "         (\"2002\", \"Cellular\", \"555-963-6544\"),\n",
    "         (\"2002\", \"Work\", \"555-812-9856\"),\n",
    "         (\"3003\", \"Cellular\", \"555-987-6541\")]\n",
    "nameColumns5 = [\"Customer ID\", \"Type\", \"Phone Number\"]\n",
    "df5 = spark.createDataFrame(data=rows5, schema = nameColumns5)\n",
    "df5.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05b16028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----+----+\n",
      "|Customer ID|Cellular|Home|Work|\n",
      "+-----------+--------+----+----+\n",
      "|       1001|Cellular|Home|Work|\n",
      "|       2002|Cellular|    |Work|\n",
      "|       3003|Cellular|    |    |\n",
      "+-----------+--------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.groupBy(\"Customer ID\").pivot(\"Type\").agg(f.concat_ws(\", \", f.collect_list(f.col(\"Type\")))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc2c508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d07dddc5",
   "metadata": {},
   "source": [
    "### Puzzle #8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14d713d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+-----+\n",
      "|Workflow|Case1|Case2|Case3|\n",
      "+--------+-----+-----+-----+\n",
      "|Alpha   |0    |0    |0    |\n",
      "|Bravo   |0    |1    |1    |\n",
      "|Charlie |1    |0    |0    |\n",
      "|Delta   |0    |0    |0    |\n",
      "+--------+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows8 = [('Alpha',0,0,0), \n",
    "         ('Bravo',0,1,1), \n",
    "         ('Charlie',1,0,0), \n",
    "         ('Delta',0,0,0)]\n",
    "nameColumns8 = [\"Workflow\", \"Case1\", \"Case2\", \"Case3\"]\n",
    "df8 = spark.createDataFrame(data=rows8, schema = nameColumns8)\n",
    "df8.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "985d995d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|Workflow|Passed|\n",
      "+--------+------+\n",
      "|   Alpha|     0|\n",
      "|   Bravo|     2|\n",
      "| Charlie|     1|\n",
      "|   Delta|     0|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.withColumn(\"Passed\", f.col(\"Case1\")+f.col(\"Case2\")+f.col(\"Case3\")).select(\"Workflow\", \"Passed\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66539575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0661a64",
   "metadata": {},
   "source": [
    "### Puzzle #13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43c78e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "985ae58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|InventoryDate|QuantityAdjustment|\n",
      "+-------------+------------------+\n",
      "|7/1/2018     |100               |\n",
      "|7/2/2018     |75                |\n",
      "|7/3/2018     |-150              |\n",
      "|7/4/2018     |50                |\n",
      "|7/5/2018     |-75               |\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows13 = [('7/1/2018',100),\n",
    "          ('7/2/2018',75), \n",
    "          ('7/3/2018',-150), \n",
    "          ('7/4/2018',50),\n",
    "          ('7/5/2018',-75)]\n",
    "nameColumns13 = [\"InventoryDate\", \"QuantityAdjustment\"]\n",
    "df13 = spark.createDataFrame(data=rows13, schema = nameColumns13)\n",
    "df13.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e8a93729",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window.orderBy(\"InventoryDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2df9443b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+---------+\n",
      "|InventoryDate|QuantityAdjustment|Inventory|\n",
      "+-------------+------------------+---------+\n",
      "|     7/1/2018|               100|      100|\n",
      "|     7/2/2018|                75|       75|\n",
      "|     7/3/2018|              -150|     -150|\n",
      "|     7/4/2018|                50|       50|\n",
      "|     7/5/2018|               -75|      -75|\n",
      "+-------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df13.withColumn(\"Inventory\", f.lag(\"QuantityAdjustment\",0).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a855ca49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05876c63",
   "metadata": {},
   "source": [
    "### Puzzle #15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "879b50cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|InventoryDate|QuantityAdjustment|\n",
      "+-------------+------------------+\n",
      "|1            |SELECT            |\n",
      "|2            |Product,          |\n",
      "|3            |UnitPrice,        |\n",
      "|4            |EffectiveDate     |\n",
      "|5            |FROM              |\n",
      "|6            |Products          |\n",
      "|7            |WHERE             |\n",
      "|8            |UnitPrice         |\n",
      "|9            |> 100             |\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows15 = [(1,'SELECT'),\n",
    "          (2,'Product,'),\n",
    "          (3,'UnitPrice,'),\n",
    "          (4,'EffectiveDate'),\n",
    "          (5,'FROM'),\n",
    "          (6,'Products'),\n",
    "          (7,'WHERE'),\n",
    "          (8,'UnitPrice'),\n",
    "          (9,'> 100')]\n",
    "nameColumns15 = [\"InventoryDate\", \"QuantityAdjustment\"]\n",
    "df15 = spark.createDataFrame(data=rows15, schema = nameColumns15)\n",
    "df15.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4fc29c27",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "grouping expressions sequence is empty, and 'InventoryDate' is not an aggregate function. Wrap '(concat_ws(', ', collect_list(QuantityAdjustment)) AS All)' in windowing function(s) or wrap 'InventoryDate' in first() (or first_value) if you don't care which value you get.;\nAggregate [InventoryDate#545L, QuantityAdjustment#546, concat_ws(, , collect_list(QuantityAdjustment#546, 0, 0)) AS All#566]\n+- LogicalRDD [InventoryDate#545L, QuantityAdjustment#546], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf15\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAll\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat_ws\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQuantityAdjustment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQuantityAdjustment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:3036\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   3034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   3035\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 3036\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: grouping expressions sequence is empty, and 'InventoryDate' is not an aggregate function. Wrap '(concat_ws(', ', collect_list(QuantityAdjustment)) AS All)' in windowing function(s) or wrap 'InventoryDate' in first() (or first_value) if you don't care which value you get.;\nAggregate [InventoryDate#545L, QuantityAdjustment#546, concat_ws(, , collect_list(QuantityAdjustment#546, 0, 0)) AS All#566]\n+- LogicalRDD [InventoryDate#545L, QuantityAdjustment#546], false\n"
     ]
    }
   ],
   "source": [
    "df15.withColumn('All', f.concat_ws(\", \", f.collect_list(f.col(\"QuantityAdjustment\")).alias(\"QuantityAdjustment\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e92f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
